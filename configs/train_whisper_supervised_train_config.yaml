hf_training_args:
  do_train: True
  do_eval: True

  max_steps: 1_000_000
  optim: "adamw_torch"
  learning_rate: 0.00002 # TODO
  warmup_steps: 500 # TODO

  # TODO all of these
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  per_device_eval_batch_size: 1
  eval_accumulation_steps: 8

  # TODO all of these as well
  dataloader_num_workers: 16
  dataloader_pin_memory: True
  dataloader_drop_last: False

  logging_steps: 10 # TODO
  report_to: "wandb"
  remove_unused_columns: False # Don't touch this

  # TODO
  fp16: False

  metric_for_best_model: "sacrebleu" # TODO change
  greater_is_better: True # TODO change according to metric

  load_best_model_at_end: True
  predict_with_generate: True # NEVER TOUCH THIS
  generation_num_beams: 1 # TODO
  generation_max_length: 80
  evaluation_strategy: "steps"
  eval_steps: 10 # TODO

  save_strategy: "steps"
  save_steps: 500 # TODO
  save_total_limit: 10 # TODO


architecture:
  name: "openai/whisper-large-v2" # TODO
  use_pretrained_whisper_encoder: True
  use_pretrained_whisper_decoder: True

early_stopping:
  should_early_stop: False
  early_stopping_patience: null
  early_stopping_threshold: null

# TODO WHOLE THIS
logging:
  log_preds_every_n_steps: 100
  # these are per dataset
  log_preds_num_train: 4
  log_preds_num_valid: 8

# TODO WHOLE THIS
dataset_mix:
  weights: 
    clotho: 0.0
    audioset: 1.0
    audiocaps: 1.0
  limit_val_split:
    clotho: null
    audioset: 200
    audiocaps: null
