hf_training_args:
  do_train: True

  max_steps: 100_000
  optim: "adamw_torch"
  learning_rate: 0.00004
  warmup_steps: 500

  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2

  dataloader_num_workers: 16
  dataloader_pin_memory: True
  dataloader_drop_last: False
  dataloader_persistent_workers: True
  dataloader_prefetch_factor: 3

  logging_steps: 1
  report_to: "wandb"
  remove_unused_columns: False # NEVER TOUCH THIS

  fp16: True

  predict_with_generate: True # NEVER TOUCH THIS
  generation_num_beams: 1
  generation_max_length: 80

  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 5


architecture:
  name: "openai/whisper-tiny"
  use_pretrained_whisper_encoder: True
  use_pretrained_whisper_decoder: True

early_stopping:
  should_early_stop: False
  early_stopping_patience: null
  early_stopping_threshold: null

logging:
  log_preds_every_n_steps: 50
  # these are per dataset
  log_preds_num_train: 8
  log_preds_num_valid: 32
