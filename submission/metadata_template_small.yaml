# Submission information for task 6, subtask A
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label following way to avoid
  # overlapping codes among submissions
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: kadlcik_jku_task6a_2
  #
  # Submission name
  # This name will be used in the results tables when space permits
  name: Careless Whisper Small
  #
  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
abbreviation: CarelessWhisperSmall

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
authors:
  # First author
  - lastname: Kadlčík
    firstname: Marek
    email: kadlcik@mail.muni.cz               # Contact email address
    corresponding: true                         # Mark true for one of the authors

    # Affiliation information for the author
    affiliation:
      abbreviation: JKU
      institute: Institute of Computational Perception
      location: Linz, Austria

  # Second author
  - lastname: Hájek
    firstname: Adam
    email: ahajek@mail.muni.cz               # Contact email address
    corresponding: true                         # Mark true for one of the authors

    # Affiliation information for the author
    affiliation:
      abbreviation: JKU
      institute: Institute of Computational Perception
      location: Linz, Austria
  
  # Third author
  - lastname: Kieslich
    firstname: Jürgen
    email: j.kieslich@gmx.at               # Contact email address
    corresponding: false                         

    # Affiliation information for the author
    affiliation:
      abbreviation: JKU
      institute: Institute of Computational Perception
      location: Linz, Austria

  # Fourth author
  - lastname: Winiecki
    firstname: Radosɫaw
    email: radoslaw.winiecki@student.put.poznan.pl
    corresponding: false                         

    # Affiliation information for the author
    affiliation:
      abbreviation: JKU
      institute: Institute of Computational Perception
      location: Linz, Austria


# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  description:

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 16kHz

    # Acoustic representation
    # Here you should indicate what can or audio representation
    # you used. If your system used hand-crafted features (e.g.
    # mel band energies), then you can do
    #
    # `acoustic_features: mel energies`
    #
    # Else, if you used some pre-trained audio feature extractor, 
    # you can indicate the name of the system, for example
    #
    # `acoustic_features: audioset`
    acoustic_features: WhisperFeatureExtractor

    # Word embeddings
    # Here you can indicate how you treated word embeddings.
    # If your method learned its own word embeddings (i.e. you
    # did not used any pre-trained word embeddings) then you can
    # do
    #
    # `word_embeddings: learned`
    #  
    # Else, specify the pre-trained word embeddings that you used
    # (e.g. Word2Vec, BERT, etc).
    word_embeddings: Whisper

    # Data augmentation methods
    # e.g. mixup, time stretching, block mixing, pitch shifting, ...
    data_augmentation: ["gaussian noise", "time shifting", "gain adjusting"]

    # Method scheme
    # Here you should indicate the scheme of the method that you
    # used. For example
    machine_learning_method: encoder-decoder

    # Learning scheme
    # Here you should indicate the learning scheme. 
    # For example, you could specify either
    # supervised, self-supervised, or even 
    # reinforcement learning. 
    learning_scheme: supervised

    # Ensemble
    # Here you should indicate if you used ensemble
    # of systems or not.
    ensemble: No

    # Audio modelling
    # Here you should indicate the type of system used for
    # audio modelling. For example, if you used some stacked CNNs, then
    # you could do
    #
    # audio_modelling: cnn
    #
    # If you used some pre-trained system for audio modelling,
    # then you should indicate the system used (e.g. COALA, COLA,
    # transfomer).
    audio_modelling: transformer

    # Word modelling
    # Similarly, here you should indicate the type of system used
    # for word modelling. For example, if you used some RNNs,
    # then you could do
    #
    # word_modelling: rnn
    #
    # If you used some pre-trained system for word modelling,
    # then you should indicate the system used (e.g. transfomer).
    word_modelling: transformer

    # Loss function
    # Here you should indicate the loss fuction that you employed.
    loss_function: crossentropy

    # Optimizer
    # Here you should indicate the name of the optimizer that you
    # used. 
    optimizer: adamw

    # Learning rate
    # Here you should indicate the learning rate of the optimizer
    # that you used.
    learning_rate: 4e-6

    # Gradient clipping
    # Here you should indicate if you used any gradient clipping. 
    # You do this by indicating the value used for clipping. Use
    # 0 for no clipping.
    gradient_clipping: 1.0

    # Gradient norm
    # Here you should indicate the norm of the gradient that you
    # used for gradient clipping. This field is used only when 
    # gradient clipping has been employed.
    gradient_norm: 1.0

    # Metric monitored
    # Here you should report the monitored metric
    # for optimizing your method. For example, did you
    # monitored the loss on the validation data (i.e. validation
    # loss)? Or you monitored the SPIDEr metric? Maybe the training
    # loss?
    metric_monitored: SPIDEr

  # System complexity, meta data provided here will be used to evaluate
  # submitted systems from the computational load perspective.
  complexity:
    # Total amount of parameters used in the acoustic model.
    # For neural networks, this information is usually given before training process
    # in the network summary.
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value (do not use comma for thousands-separator).
    total_parameters: 244000000

  # List of external datasets used in the submission.
  # Development dataset is used here only as example, list only external datasets
  external_datasets:
    # Dataset name
    - name: Clotho

      # Dataset access url
      url: https://doi.org/10.5281/zenodo.3490683

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 5

      # Total amount of examples used
      total_audio_length: 24430

      # Used for (e.g. audio_modelling, word_modelling, audio_and_word_modelling)
      used_for: audio_and_word_modelling


    # Dataset name
    - name: AudioCaps

      # Dataset access url
      url: https://doi.org/10.18653/v1/N19-1011

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: Yes

      # Number of captions per audio
      nb_captions_per_audio: 1

      # Total amount of examples used
      total_audio_length: 46700

      # Used for (e.g. audio_modelling, word_modelling, audio_and_word_modelling)
      used_for: audio_and_word_modelling


    # Dataset name
    - name: AudioSet

      # Dataset access url
      url: https://doi.org/10.1109/ICASSP.2017.7952261

      # Has audio:
      has_audio: Yes

      # Has images
      has_images: No

      # Has video
      has_video: No

      # Has captions
      has_captions: No

      # Number of captions per audio
      nb_captions_per_audio: 0

      # Total amount of examples used
      total_audio_length: 134002

      # Used for (e.g. audio_modelling, word_modelling, audio_and_word_modelling)
      used_for: audio_and_word_modelling


  # URL to the source code of the system [optional]
  source_code: https://github.com/audio-captioning/dcase-2023-baseline

# System results
results:
  development_evaluation:
    # System results for development evaluation split.
    # Full results are not mandatory, however, they are highly recommended
    # as they are needed for through analysis of the challenge submissions.
    # If you are unable to provide all results, also incomplete
    # results can be reported.
    meteor: 0.3781
    cider: 0.4142
    spice: 0.1234
    spider: 0.2687
    spiderfl: !!null